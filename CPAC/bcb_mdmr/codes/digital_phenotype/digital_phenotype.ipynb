{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "mnt_dir = \"/mnt/NAS2-2/data/\"\n",
    "dp_dir = f\"{mnt_dir}/SAD_gangnam_DP/\"\n",
    "feature_dir = f\"{dp_dir}/dp_features/\"\n",
    "feature_path = Path(feature_dir)\n",
    "dp_path = Path(dp_dir)\n",
    "\n",
    "subfolders = [f for f in feature_path.iterdir() if f.is_dir()]\n",
    "\n",
    "dp_fmri_subject = pd.read_csv(dp_path/\"dp_fmri_subject.csv\")\n",
    "dp_types = [\"app\", \"call\", \"light\", \"location\", \"screen\"]\n",
    "group_types = [\"HC\", \"SAD\"]\n",
    "week_types = [\"weekdays\", \"weekends\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fmri_log_data(dp_fmri_subject, log_data):\n",
    "    from datetime import datetime, timedelta\n",
    "    import pandas as pd\n",
    "\n",
    "    # 날짜 변환 중 오류가 발생한 피험자를 추적하기 위한 리스트\n",
    "    date_conversion_errors = []\n",
    "\n",
    "    # 최종 결과를 저장할 리스트\n",
    "    results = []\n",
    "\n",
    "    # subjNum의 결측치 확인 및 제거\n",
    "    dp_fmri_subject = dp_fmri_subject.dropna(subset=['subjNum'])\n",
    "    dp_fmri_subject['subjNum'] = dp_fmri_subject['subjNum'].astype(int)\n",
    "\n",
    "    log_data = log_data.dropna(subset=['subjNum'])\n",
    "    log_data['subjNum'] = log_data['subjNum'].astype(int)\n",
    "\n",
    "    # 'fmri_date' 열을 datetime 형식으로 변환\n",
    "    dp_fmri_subject['fmri_date'] = pd.to_datetime(dp_fmri_subject['fmri_date'], errors='coerce')\n",
    "\n",
    "    # 'local_segment_start_datetime' 열을 datetime 형식으로 변환\n",
    "    log_data['local_segment_start_datetime'] = pd.to_datetime(log_data['local_segment_start_datetime'], errors='coerce')\n",
    "\n",
    "    # subjNum, fmri_code, fmri_date별로 그룹화하여 처리\n",
    "    grouped_subjects = dp_fmri_subject.groupby(['subjNum', 'fmri_code', 'fmri_date'])\n",
    "\n",
    "    for (subj_num, fmri_code, fmri_date), group in grouped_subjects:\n",
    "        # fmri_date가 제대로 변환되었는지 확인\n",
    "        if pd.isnull(fmri_date):\n",
    "            date_conversion_errors.append(subj_num)\n",
    "            continue\n",
    "\n",
    "        # 해당 피험자의 로그 데이터를 필터링\n",
    "        subj_log_data = log_data[log_data['subjNum'] == subj_num]\n",
    "\n",
    "        # 로그 데이터의 날짜가 제대로 변환되었는지 확인하고 NaT 제거\n",
    "        subj_log_data = subj_log_data.dropna(subset=['local_segment_start_datetime'])\n",
    "        subj_log_dates = subj_log_data['local_segment_start_datetime']\n",
    "\n",
    "        if subj_log_dates.empty:\n",
    "            # 로그 데이터가 없을 경우, 해당 피험자를 건너뜁니다\n",
    "            continue\n",
    "\n",
    "        # 피험자의 전체 날짜 범위 계산\n",
    "        min_date = subj_log_dates.min()\n",
    "        max_date = subj_log_dates.max()\n",
    "\n",
    "        # 윈도우 크기 설정 (2주)\n",
    "        window_size = timedelta(weeks=2)\n",
    "\n",
    "        # 모든 윈도우의 시작 날짜 생성\n",
    "        window_start_dates = []\n",
    "        current_start_date = min_date\n",
    "        while current_start_date <= max_date:\n",
    "            window_start_dates.append(current_start_date)\n",
    "            current_start_date += window_size\n",
    "\n",
    "        # 각 윈도우에 대해 데이터 계산\n",
    "        window_results = []\n",
    "        for ws_date in window_start_dates:\n",
    "            we_date = ws_date + window_size\n",
    "            window_data = subj_log_data[(subj_log_data['local_segment_start_datetime'] >= ws_date) &\n",
    "                                        (subj_log_data['local_segment_start_datetime'] < we_date)]\n",
    "\n",
    "            if not window_data.empty:\n",
    "                numeric_data = window_data.select_dtypes(include='number').mean()\n",
    "            else:\n",
    "                numeric_columns = log_data.select_dtypes(include='number').columns\n",
    "                numeric_data = pd.Series(0, index=numeric_columns)\n",
    "\n",
    "            result_row = {\n",
    "                'subjNum': subj_num,\n",
    "                'fmri_code': fmri_code,\n",
    "                'fmri_date': fmri_date,\n",
    "                'window_start_date': ws_date,\n",
    "                'window_end_date': we_date\n",
    "            }\n",
    "            result_row.update(numeric_data.to_dict())\n",
    "            window_results.append(result_row)\n",
    "\n",
    "        # 윈도우의 시작 날짜 중 fmri_date와 ±1주일 이내인 윈도우를 찾음\n",
    "        fmri_date = pd.to_datetime(fmri_date)\n",
    "        candidates = [wr for wr in window_results if abs((wr['window_start_date'] - fmri_date).days) <= 7]\n",
    "        if candidates:\n",
    "            # 가장 가까운 윈도우를 찾음\n",
    "            closest_window = min(candidates, key=lambda x: abs((x['window_start_date'] - fmri_date).days))\n",
    "            chunk_num_0_start_date = closest_window['window_start_date']\n",
    "        else:\n",
    "            # 해당하는 윈도우가 없으면 fmri_date와 가장 가까운 윈도우를 찾음\n",
    "            closest_window = min(window_results, key=lambda x: abs((x['window_start_date'] - fmri_date).days))\n",
    "            chunk_num_0_start_date = closest_window['window_start_date']\n",
    "\n",
    "        # 각 윈도우에 chunk_num 할당\n",
    "        for wr in window_results:\n",
    "            # 윈도우의 시작 날짜와 chunk_num 0 윈도우의 시작 날짜 차이를 계산하여 chunk_num 할당\n",
    "            days_diff = (wr['window_start_date'] - chunk_num_0_start_date).days\n",
    "            wr['chunk_num'] = days_diff // 14  # 14일 단위로 나눠서 chunk_num 계산\n",
    "            results.append(wr)\n",
    "\n",
    "    # 결과를 데이터프레임으로 변환\n",
    "    final_window_df = pd.DataFrame(results)\n",
    "\n",
    "    # window_start_date나 window_end_date가 NaT인 행을 제거\n",
    "    final_window_df = final_window_df.dropna(subset=['window_start_date', 'window_end_date'])\n",
    "\n",
    "    # subjNum을 정수형으로 변환하여 소수점 제거\n",
    "    final_window_df['subjNum'] = final_window_df['subjNum'].astype(int)\n",
    "\n",
    "    # chunk_num을 정렬\n",
    "    final_window_df = final_window_df.sort_values(by=['subjNum', 'chunk_num'])\n",
    "\n",
    "    # 최종 데이터프레임과 날짜 변환 오류가 발생한 피험자 목록 반환\n",
    "    return final_window_df, date_conversion_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dp_type in dp_types:\n",
    "    for group in group_types:\n",
    "        for week_type in week_types:\n",
    "            if group == \"HC\":\n",
    "                group_specifier = \"hc\"\n",
    "            elif group == \"SAD\":\n",
    "                group_specifier = \"sad\"\n",
    "            intput_file_name = f\"{dp_type}_{week_type}_{group_specifier}.csv\"\n",
    "            output_file_name = f\"{dp_type}_{week_type}_{group_specifier}_window.csv\"\n",
    "            target_df = pd.read_csv(feature_path / dp_type / group / intput_file_name)\n",
    "            final_window_df, date_conversion_errors = process_fmri_log_data(dp_fmri_subject, target_df)\n",
    "            final_window_df.to_csv(feature_path / dp_type / group / output_file_name)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n",
      "app weekdays\n",
      "{'subjNum': 0.003875052035449824, 'phone_applications_foreground_rapids_countepisodesns_custom': 0.025383931223577936, 'phone_applications_foreground_rapids_countepisodecommunication_custom_category': 0.018444863650960288, 'phone_applications_foreground_rapids_countepisodesocialmedia_custom_category': 0.020621960147809763, 'phone_applications_foreground_rapids_maxdurationall': 0.046328856350739904, 'phone_applications_foreground_rapids_counteventsns_custom': 0.025383931223577936, 'phone_applications_foreground_rapids_counteventcommunication_custom_category': 0.018444863650960288, 'phone_applications_foreground_rapids_counteventsocialmedia_custom_category': 0.020621960147809763, 'phone_applications_foreground_rapids_frequencyentropysns_custom': 0.028037361996839576, 'phone_applications_foreground_rapids_counteventratio_sns': 0.03431836516697072, 'phone_applications_foreground_rapids_counteventratio_communication': 0.02529162314092532, 'phone_applications_foreground_rapids_counteventratio_socialmedia': 0.030101255762164825}\n",
      "108\n",
      "app weekends\n",
      "{'subjNum': 0.003875052035449824, 'phone_applications_foreground_rapids_countepisodeall': 0.028290896518498775, 'phone_applications_foreground_rapids_countepisodechat_custom': 0.046041985940099506, 'phone_applications_foreground_rapids_countepisodesns_custom': 0.01756447526138131, 'phone_applications_foreground_rapids_countepisodecommunication_custom_category': 0.012384593914288446, 'phone_applications_foreground_rapids_countepisodesocialmedia_custom_category': 0.012881327210305122, 'phone_applications_foreground_rapids_counteventall': 0.028290896518498775, 'phone_applications_foreground_rapids_counteventchat_custom': 0.046041985940099506, 'phone_applications_foreground_rapids_counteventsns_custom': 0.01756447526138131, 'phone_applications_foreground_rapids_counteventcommunication_custom_category': 0.012384593914288446, 'phone_applications_foreground_rapids_counteventsocialmedia_custom_category': 0.012881327210305122, 'phone_applications_foreground_rapids_frequencyentropysns_custom': 0.035094325647198034, 'phone_applications_foreground_rapids_counteventratio_sns': 0.041765602538178265, 'phone_applications_foreground_rapids_counteventratio_communication': 0.04895550732713195}\n",
      "33\n",
      "call weekdays\n",
      "{'subjNum': 0.005072331248884796}\n",
      "33\n",
      "call weekends\n",
      "{'subjNum': 0.005072331248884796}\n",
      "16\n",
      "light weekdays\n",
      "{'subjNum': 0.0019374774520536397}\n",
      "16\n",
      "light weekends\n",
      "{'subjNum': 0.0019374774520536397}\n",
      "19\n",
      "location weekdays\n",
      "{'subjNum': 0.0023146250274054102}\n",
      "19\n",
      "location weekends\n",
      "{'subjNum': 0.0023146250274054102}\n",
      "11\n",
      "screen weekdays\n",
      "{'subjNum': 0.003875052035449824, 'phone_screen_rapids_stddurationunlock': 0.04307459923974477}\n",
      "11\n",
      "screen weekends\n",
      "{'subjNum': 0.003875052035449824}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/changbae/anaconda3/envs/fmrienv/lib/python3.9/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/changbae/anaconda3/envs/fmrienv/lib/python3.9/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "def find_significant_features(data1, data2, chunk_num=0, p_value_threshold=0.05):\n",
    "    \"\"\"\n",
    "    This function compares two datasets, applies a t-test on matching numeric columns,\n",
    "    and returns the features where p < 0.05.\n",
    "    \n",
    "    Parameters:\n",
    "    - file1: path to the first CSV file (e.g., SAD group)\n",
    "    - file2: path to the second CSV file (e.g., HC group)\n",
    "    - chunk_num: chunk number to filter on (default is 0)\n",
    "    - p_value_threshold: the p-value threshold to consider a feature significant (default is 0.05)\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary of significant features and their p-values.\n",
    "    \"\"\"\n",
    "    # Filter for the specified chunk number\n",
    "    data1_chunk = data1[data1['chunk_num'] == chunk_num]\n",
    "    data2_chunk = data2[data2['chunk_num'] == chunk_num]\n",
    "    \n",
    "    # Extract numeric columns\n",
    "    numeric_columns1 = data1_chunk.select_dtypes(include='number').columns\n",
    "    numeric_columns2 = data2_chunk.select_dtypes(include='number').columns\n",
    "    \n",
    "    # Ensure both datasets have matching numeric columns\n",
    "    common_columns = numeric_columns1.intersection(numeric_columns2)\n",
    "    print(len(common_columns))\n",
    "    # Extract only the relevant numeric columns for the chunk\n",
    "    data1_numeric = data1_chunk[common_columns]\n",
    "    data2_numeric = data2_chunk[common_columns]\n",
    "    \n",
    "    # Perform t-tests and store p-values\n",
    "    p_values = {}\n",
    "    for col in common_columns:\n",
    "        stat, p_val = ttest_ind(data1_numeric[col], data2_numeric[col], nan_policy='omit')\n",
    "        p_values[col] = p_val\n",
    "    \n",
    "    # Filter for significant features\n",
    "    significant_metrics = {k: v for k, v in p_values.items() if v < p_value_threshold}\n",
    "    \n",
    "    return significant_metrics\n",
    "\n",
    "for dp_type in dp_types:\n",
    "    for week_type in week_types:\n",
    "        \n",
    "        sad_data_file = f\"{dp_type}_{week_type}_sad_window.csv\"\n",
    "        hc_data_file = f\"{dp_type}_{week_type}_hc_window.csv\"\n",
    "        sad_data_df = pd.read_csv(feature_path / dp_type / \"SAD\" / sad_data_file)\n",
    "        hc_data_df = pd.read_csv(feature_path / dp_type / \"HC\" / hc_data_file)\n",
    "        significant_features = find_significant_features(sad_data_df, hc_data_df)\n",
    "        print(f\"{dp_type} {week_type}\")\n",
    "        print(significant_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmrienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
